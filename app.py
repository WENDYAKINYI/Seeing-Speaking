import streamlit as st
import torch
from torchvision import transforms
from PIL import Image
import openai
import requests
from io import BytesIO
import numpy as np
from utils import (
    load_baseline_model,
    generate_baseline_caption,
    enhance_with_openai,
    load_image,
    preprocess_image
)

# --- Configuration ---
st.set_page_config(page_title="Caption Showdown", layout="wide")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- Model Loading ---
@st.cache_resource
def load_baseline_model():
    model_files = {
        "encoder": "https://huggingface.co/weakyy/image-captioning-baseline-model/resolve/main/encoder.pth",
        "decoder": "https://huggingface.co/weakyy/image-captioning-baseline-model/resolve/main/decoder.pth",
        "word2idx": "https://huggingface.co/weakyy/image-captioning-baseline-model/resolve/main/word2idx.pkl",
        "idx2word": "https://huggingface.co/weakyy/image-captioning-baseline-model/resolve/main/idx2word.pkl"
    }

    # Download files
    def download_file(url):
        response = requests.get(url)
        return BytesIO(response.content)
    
    # Initialize models first
    encoder = EncoderCNN().eval().to(device)
    decoder = DecoderRNN(
        attention_dim=256,
        embed_dim=256,
        decoder_dim=512,
        vocab_size=10004  # Update with your actual vocab size
    ).eval().to(device)
    
    # Load weights
    enc_file = download_file(model_files["encoder"])
    dec_file = download_file(model_files["decoder"])
    if enc_file and dec_file:
        encoder.load_state_dict(torch.load(enc_file, map_location=device))
        decoder.load_state_dict(torch.load(dec_file, map_location=device))
    
    # Load vocab
    vocab = {
        "word2idx": torch.load(download_file(model_files["word2idx"]), map_location='cpu'),
        "idx2word": torch.load(download_file(model_files["idx2word"]), map_location='cpu')
    }
    
    return encoder, decoder, vocab

# --- Image Processing ---
# Generate baseline caption
result = generate_baseline_caption(
    image_tensor, 
    encoder, 
    decoder, 
    vocab,
    beam_size=5
)

# Enhance with OpenAI
enhanced = enhance_with_openai(result["caption"])

# --- UI Layout ---
st.title("Vision to Text: Baseline üÜö OpenAI")
st.caption("Compare a custom CNN-RNN model against OpenAI GPT-3.5")

# Sidebar
with st.sidebar:
    st.header("Settings")
    openai_enabled = st.toggle("Enable OpenAI", True)
    st.divider()
    st.markdown("""
    **Model Details**  
    - Baseline: ResNet50 + Attention LSTM  
    - OpenAI: GPT-3.5 Turbo  
    [View Baseline Model](https://huggingface.co/weakyy/image-captioning-baseline-model)
    """)

# Main Content
uploaded_file = st.file_uploader("Upload an image", type=["jpg", "png", "jpeg"])
example_images = {
    "Beach": "https://images.unsplash.com/photo-1507525428034-b723cf961d3e",
    "Dog": "https://images.unsplash.com/photo-1561037404-61cd46aa615b",
    "Food": "https://images.unsplash.com/photo-1565958011703-72f8583c2708"
}

if not uploaded_file:
    selected = st.selectbox("Or try an example:", list(example_images.keys()))
    image = Image.open(requests.get(example_images[selected], stream=True).raw)
else:
    image = Image.open(uploaded_file).convert("RGB")

st.image(image, caption="Input Image", use_column_width=True)

# Generate Captions
encoder, decoder, vocab = load_baseline_model()
image_tensor = preprocess_image(image)

col1, col2 = st.columns(2)

with col1:
    st.subheader("üß† Baseline Model")
    with st.spinner("Generating baseline caption..."):
        baseline_result = generate_baseline_caption(image_tensor, encoder, decoder, vocab)
        st.success(baseline_result["caption"])
        st.caption(f"Confidence: {baseline_result['confidence']:.0%}")
        
        # Feedback
        st.write("Rate this caption:")
        if st.button("üëç", key="like_baseline"):
            st.toast("Thanks for your feedback!")
        st.button("üëé", key="dislike_baseline")

with col2:
    st.subheader("‚ú® OpenAI Enhanced")
    if openai_enabled and 'openai_key' in st.secrets:
        with st.spinner("Enhancing with GPT-3.5..."):
            enhanced = enhance_with_openai(baseline_result["caption"])
            st.success(enhanced)
            
            # Feedback
            st.write("Rate this enhancement:")
            if st.button("üëç", key="like_openai"):
                st.toast("Thanks for your feedback!")
            st.button("üëé", key="dislike_openai")
    elif not openai_enabled:
        st.info("Enable OpenAI in sidebar")
    else:
        st.error("Add OpenAI key in secrets.toml")

# Performance Metrics
with st.expander("üìä Performance Comparison"):
    st.table({
        "Model": ["Baseline", "OpenAI Enhanced"],
        "BLEU-4": [0.42, 0.61],
        "Inference Time": ["1.2s", "2.8s"]
    })

# Footer
st.divider()
st.caption("""
‚ö° **Pro Tip**: Click images to zoom | 
[GitHub Repo](https://github.com/your-repo) | 
[Model Card](https://huggingface.co/weakyy/image-captioning-baseline-model)
""")
